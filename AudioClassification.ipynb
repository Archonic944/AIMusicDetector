{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install resampy\n",
    "!pip3 install librosa\n",
    "!pip3 install joblib\n",
    "!pip3 install tensorflow\n",
    "!pip3 install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgaOh7v01Duq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "LOAD_MODEL = True # Whether to train the model, or just evaluate based on 21Jan2025.h5\n",
    "\n",
    "AUDIO_CONTENT_DIR = 'content/audio'       # Original audio directory\n",
    "FEATURES_CONTENT_DIR = 'content/features' # Pre-calculated features (MFCCs)\n",
    "classes = [\"human\", \"ai\"]\n",
    "\n",
    "SEGMENT_DURATION = 1.0  # Duration of each audio segment in seconds\n",
    "HOP_LENGTH = 1.0        # Hop length in seconds between segments (should usually be the same as segment duration)\n",
    "PAD_LEN = 40           # Fixed length for MFCC padding/truncating\n",
    "MFCC_COUNT = 40        # Number of MFCCs per set (set this according to segment duration)\n",
    "\n",
    "# Volume is randomized during normalization between these two values\n",
    "MAX_TARGET_DB = -20 \n",
    "MIN_TARGET_DB = -40 \n",
    "\n",
    "# Utils\n",
    "def db_to_linear(db):\n",
    "    return 10 ** (db / 20)\n",
    "def linear_to_db(rms):\n",
    "    return 20 * np.log10(rms) if rms > 0 else -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BAbfUNl1KYf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the chop_audio Function\n",
    "def chop_audio(audio, sample_rate=22050, segment_duration=SEGMENT_DURATION, hop_length=HOP_LENGTH):\n",
    "    \"\"\"\n",
    "    Splits an audio array into multiple segments.\n",
    "\n",
    "    Parameters:\n",
    "    - audio (np.ndarray): The original audio signal.\n",
    "    - sample_rate (int): The sample rate of the audio.\n",
    "    - segment_duration (float): Duration of each segment in seconds.\n",
    "    - hop_duration (float): Time to skip between segments in seconds.\n",
    "\n",
    "    Returns:\n",
    "    - List[np.ndarray]: List of audio segments.\n",
    "    \"\"\"\n",
    "    segment_length_samples = int(segment_duration * sample_rate)\n",
    "    hop_length_samples = int(hop_length * sample_rate)\n",
    "    segments = []\n",
    "\n",
    "    for start in range(0, len(audio) - segment_length_samples + 1, hop_length_samples):\n",
    "        end = start + segment_length_samples\n",
    "        segment = audio[start:end]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TI6QH0yP1LnV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(audio, sample_rate=22050, pad_len=PAD_LEN):\n",
    "    \"\"\"\n",
    "    Extracts MFCC features from an audio segment and pads/truncates them to a fixed length.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio (np.ndarray): The audio segment.\n",
    "    - sample_rate (int): The sample rate of the audio.\n",
    "    - pad_len (int): The fixed length for padding/truncating.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: MFCC feature array of shape (40, PAD_LEN).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        \n",
    "        # Calculate padding or truncating\n",
    "        pad_width = pad_len - mfccs.shape[1]\n",
    "        \n",
    "        if pad_width > 0:\n",
    "            # Pad MFCCs with zeros on the right\n",
    "            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            # Truncate MFCCs to the desired length\n",
    "            mfccs = mfccs[:, :pad_len]\n",
    "        \n",
    "        return mfccs\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature extraction: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_loudness(audio, max_db=MAX_TARGET_DB, min_db=MIN_TARGET_DB, debug=False):\n",
    "    \"\"\"\n",
    "    Normalizes the loudness of an audio segment to a random target RMS between min_db and max_db,\n",
    "    ensuring that no clipping occurs. Logs the difference from the original dB if clipping is prevented.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio (np.ndarray): The audio segment.\n",
    "    - max_db (float): Maximum target RMS in decibels.\n",
    "    - min_db (float): Minimum target RMS in decibels.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: The normalized audio segment.\n",
    "    \"\"\"\n",
    "    target_db = random.uniform(min_db, max_db)\n",
    "    target_rms = db_to_linear(target_db)\n",
    "    \n",
    "    current_rms = np.sqrt(np.mean(audio**2))\n",
    "    if current_rms == 0:\n",
    "        return audio  # Avoid division by zero\n",
    "    \n",
    "    desired_scaling = target_rms / current_rms\n",
    "    max_sample = np.max(np.abs(audio * desired_scaling))\n",
    "    \n",
    "    if max_sample > 1.0:\n",
    "        # Calculate scaling factor to prevent clipping\n",
    "        clipping_scaling = 1.0 / np.max(np.abs(audio))\n",
    "        # Choose the smaller scaling factor\n",
    "        scaling_factor = min(desired_scaling, clipping_scaling)\n",
    "        adjusted_rms = current_rms * scaling_factor\n",
    "        adjusted_db = linear_to_db(adjusted_rms)\n",
    "        db_difference = adjusted_db - target_db\n",
    "        if debug:\n",
    "            print(f\"Clipping prevented: Original target_db={target_db:.2f} dB, \"\n",
    "              f\"Adjusted target_db={adjusted_db:.2f} dB, Difference={db_difference:.2f} dB\")\n",
    "    else:\n",
    "        scaling_factor = desired_scaling\n",
    "    \n",
    "    normalized_audio = audio * scaling_factor\n",
    "    return normalized_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_and_save_npy(file_path, label, output_dir, augment=True, force=False):\n",
    "    \"\"\"\n",
    "    Processes a single audio file: extracts MFCC features and saves them as a compressed .npz file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the audio file.\n",
    "    - label (int): Label of the audio file (0 for human, 1 for ai).\n",
    "    - output_dir (str): Directory to save the .npz feature file.\n",
    "    - augment (bool): If True, apply loudness normalization. If False, skip normalization.\n",
    "    - force (bool): If True, overwrite existing feature files.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_file = os.path.join(output_dir, f\"{base_name}.npz\")\n",
    "        \n",
    "        # Check if feature file is already saved. Does not perform additional checks like if the feature file matches the CURRENT segment length.\n",
    "        if os.path.exists(output_file) and not force:\n",
    "            print(f\"Features for '{base_name}' already exist. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast', mono=True)\n",
    "        \n",
    "        # Chop the audio into segments\n",
    "        segments = chop_audio(audio, sample_rate, SEGMENT_DURATION, HOP_LENGTH)\n",
    "        \n",
    "        # Extract features for each segment\n",
    "        features = []\n",
    "        for segment in segments:\n",
    "            if augment:\n",
    "                normalized_segment = normalize_loudness(segment, MAX_TARGET_DB, MIN_TARGET_DB)\n",
    "            else:\n",
    "                normalized_segment = segment  # Skip normalization for validation\n",
    "            mfcc = extract_features(normalized_segment, sample_rate, PAD_LEN)\n",
    "            if mfcc is not None:\n",
    "                features.append(mfcc.astype(np.float32))  # Ensure float32 for consistency\n",
    "        \n",
    "        if not features:\n",
    "            print(f\"No features extracted for '{base_name}'. Skipping saving.\")\n",
    "            return\n",
    "        \n",
    "        # Convert to numpy array and add channel dimension\n",
    "        features = np.array(features, dtype=np.float32)[..., np.newaxis]  # Shape: (num_segments, 40, PAD_LEN, 1)\n",
    "        \n",
    "        # Save as compressed .npz file\n",
    "        np.savez_compressed(output_file, features=features, label=label)\n",
    "        print(f\"Saved features for '{base_name}' to '{output_file}'\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing '{file_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FvQ8jFL1OqV",
    "outputId": "7ee81354-5cdc-4730-beee-294b27e23b64",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_features(feature_dir, classes):\n",
    "    \"\"\"\n",
    "    Loads precomputed features from training and validation directories and maps them to file names.\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_dir (str): Path to the features directory.\n",
    "    - classes (list): List of class names (e.g., [\"human\", \"ai\"]).\n",
    "    \n",
    "    Returns:\n",
    "    - features_mapping (dict): Dictionary mapping file names (without extension) to their features and labels.\n",
    "    \"\"\"\n",
    "    features_mapping = {}\n",
    "    splits = ['train', 'validation']\n",
    "    \n",
    "    for split in splits:\n",
    "        for cls in classes:\n",
    "            subdir_path = os.path.join(feature_dir, split, cls)\n",
    "            if not os.path.isdir(subdir_path):\n",
    "                print(f\"Directory '{subdir_path}' does not exist. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            label = classes.index(cls)  # 0 for human, 1 for ai\n",
    "            \n",
    "            for file in os.listdir(subdir_path):\n",
    "                if file.lower().endswith('.npz'):\n",
    "                    base_name = os.path.splitext(file)[0]\n",
    "                    if base_name in features_mapping:\n",
    "                        print(f\"Features for '{base_name}' already loaded. Skipping.\")\n",
    "                        continue\n",
    "                    \n",
    "                    file_path = os.path.join(subdir_path, file)\n",
    "                    try:\n",
    "                        with np.load(file_path) as data:\n",
    "                            features = data['features']\n",
    "                            label = int(data['label'])\n",
    "                            features_mapping[base_name] = {\n",
    "                                'features': features,  # Shape: (num_segments, 40, PAD_LEN, 1)\n",
    "                                'label': label         # 0 for human, 1 for ai\n",
    "                            }\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading '{file_path}': {e}\")\n",
    "    \n",
    "    print(f\"Total feature files loaded: {len(features_mapping)}\")\n",
    "    return features_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_all_features(force=False):\n",
    "    \"\"\"\n",
    "    Processes and saves features from both training and validation directories.\n",
    "    \n",
    "    Parameters:\n",
    "    - force (bool): If True, overwrite existing feature files.\n",
    "    \n",
    "    Returns:\n",
    "    - train_files (list): List of training file names (without extension).\n",
    "    - val_files (list): List of validation file names (without extension).\n",
    "    \"\"\"\n",
    "    classes = [\"human\", \"ai\"]\n",
    "    \n",
    "    # Define splits; used to determine file paths\n",
    "    splits = {\n",
    "        'train': True,        # Apply augmentation\n",
    "        'validation': False   # Do not apply augmentation\n",
    "    }\n",
    "    \n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    \n",
    "    for split, augment in splits.items():\n",
    "        for cls in classes:\n",
    "            cls_audio_dir = os.path.join(AUDIO_CONTENT_DIR, split, cls)\n",
    "            cls_feature_dir = os.path.join(FEATURES_CONTENT_DIR, split, cls)\n",
    "            for f in os.listdir(cls_audio_dir):\n",
    "                if f.lower().endswith(('.wav', '.mp3')):\n",
    "                    file_path = os.path.join(cls_audio_dir, f)\n",
    "                    label = classes.index(cls)\n",
    "                    process_and_save_npy(file_path, label, cls_feature_dir, augment=augment, force=force)\n",
    "                    \n",
    "                    base_name = os.path.splitext(f)[0]\n",
    "                    if split == 'train':\n",
    "                        train_files.append(base_name)\n",
    "                    else:\n",
    "                        val_files.append(base_name)\n",
    "                    \n",
    "    print(\"Feature extraction and saving complete.\")\n",
    "    print(f\"Training files: {len(train_files)}\")\n",
    "    print(f\"Validation files: {len(val_files)}\")\n",
    "    \n",
    "    return train_files, val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def verify_class_distribution(y, dataset_type='Dataset'):\n",
    "    class_labels = np.argmax(y, axis=1)\n",
    "    unique, counts = np.unique(class_labels, return_counts=True)\n",
    "    class_distribution = dict(zip(['human', 'ai'], counts))\n",
    "    print(f\"{dataset_type} class distribution: {class_distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def verify_directory_structure(feature_dir, audio_dir, classes=[\"human\", \"ai\"], splits=[\"train\", \"validation\"]):\n",
    "    \"\"\"\n",
    "    Verifies the directory structure and ensures that the number of feature files matches the number of audio files.\n",
    "    Prints details including the effective split percentages between train and validation for each class.\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_dir (str): Path to the base features directory (e.g., 'content/features').\n",
    "    - audio_dir (str): Path to the base audio directory (e.g., 'content/audio').\n",
    "    - classes (list): List of class names (e.g., [\"human\", \"ai\"]).\n",
    "    - splits (list): List of data splits (e.g., [\"train\", \"validation\"]).\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print(f\"Verifying directory structure between '{audio_dir}' and '{feature_dir}'...\\n\")\n",
    "    \n",
    "    counts = {cls: {split: {\"audio\": 0, \"features\": 0} for split in splits} for cls in classes}\n",
    "    total_counts = {cls: {\"audio\": 0, \"features\": 0} for cls in classes}\n",
    "    \n",
    "    # Flag to track overall structure validity\n",
    "    structure_valid = True\n",
    "    \n",
    "    for split in splits:\n",
    "        for cls in classes:\n",
    "            audio_split_dir = os.path.join(audio_dir, split, cls)\n",
    "            feature_split_dir = os.path.join(feature_dir, split, cls)\n",
    "            \n",
    "            # Check if directories exist\n",
    "            if not os.path.isdir(audio_split_dir):\n",
    "                print(f\"Missing audio directory: '{audio_split_dir}'\")\n",
    "                structure_valid = False\n",
    "            if not os.path.isdir(feature_split_dir):\n",
    "                print(f\"Missing feature directory: '{feature_split_dir}'\")\n",
    "                structure_valid = False\n",
    "                continue  # Skip counting if feature directory doesn't exist\n",
    "            \n",
    "            # Count audio files\n",
    "            audio_files = [f for f in os.listdir(audio_split_dir) if f.lower().endswith(('.wav', '.mp3'))]\n",
    "            audio_count = len(audio_files)\n",
    "            counts[cls][split][\"audio\"] = audio_count\n",
    "            total_counts[cls][\"audio\"] += audio_count\n",
    "            \n",
    "            # Count feature files\n",
    "            feature_files = [f for f in os.listdir(feature_split_dir) if f.lower().endswith('.npz')]\n",
    "            feature_count = len(feature_files)\n",
    "            counts[cls][split][\"features\"] = feature_count\n",
    "            total_counts[cls][\"features\"] += feature_count\n",
    "            \n",
    "            # Print counts\n",
    "            print(f\"Class '{cls}' - Split '{split}':\")\n",
    "            print(f\"  Audio files: {audio_count}\")\n",
    "            print(f\"  Feature files: {feature_count}\\n\")\n",
    "            \n",
    "            # Check if counts match\n",
    "            if audio_count != feature_count:\n",
    "                print(f\"Mismatch in counts for Class '{cls}', Split '{split}':\")\n",
    "                print(f\"  Audio files: {audio_count} vs Feature files: {feature_count}\\n\")\n",
    "                structure_valid = False\n",
    "    \n",
    "    if not structure_valid:\n",
    "        print(\"Directory structure verification failed due to missing directories or mismatched file counts.\\n\")\n",
    "    else:\n",
    "        print(\"All directories exist and file counts match.\\n\")\n",
    "    \n",
    "    print(\"Effective Split Percentages:\\n\")\n",
    "    for cls in classes:\n",
    "        train_audio = counts[cls][\"train\"][\"audio\"]\n",
    "        val_audio = counts[cls][\"validation\"][\"audio\"]\n",
    "        total_audio = total_counts[cls][\"audio\"]\n",
    "        \n",
    "        if total_audio == 0:\n",
    "            print(f\"Class '{cls}': No audio files found.\")\n",
    "            continue\n",
    "        \n",
    "        train_pct = (train_audio / total_audio) * 100\n",
    "        val_pct = (val_audio / total_audio) * 100\n",
    "        \n",
    "        print(f\"Class '{cls}':\")\n",
    "        print(f\"  Training: {train_audio} files ({train_pct:.2f}%)\")\n",
    "        print(f\"  Validation: {val_audio} files ({val_pct:.2f}%)\\n\")\n",
    "    \n",
    "    print(\"Verification Summary:\")\n",
    "    for cls in classes:\n",
    "        total_audio = total_counts[cls][\"audio\"]\n",
    "        total_features = total_counts[cls][\"features\"]\n",
    "        print(f\"Class '{cls}' - Total Audio Files: {total_audio}, Total Feature Files: {total_features}\")\n",
    "    print(\"\\nDirectory structure verification complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_val(features_mapping, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the files into training and validation sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - features_mapping (dict): Dictionary mapping file names to their features and labels.\n",
    "    - val_size (float): Proportion of the dataset to include in the validation split.\n",
    "    - random_state (int): Seed used by the random number generator.\n",
    "    \n",
    "    Returns:\n",
    "    - train_files (list): List of file names for training.\n",
    "    - val_files (list): List of file names for validation.\n",
    "    \"\"\"\n",
    "    file_names = list(features_mapping.keys())\n",
    "    labels = [features_mapping[file]['label'] for file in file_names]\n",
    "    \n",
    "    train_files, val_files = train_test_split(\n",
    "        file_names,\n",
    "        test_size=val_size,\n",
    "        stratify=labels,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"Training files: {len(train_files)}\")\n",
    "    print(f\"Validation files: {len(val_files)}\")\n",
    "    \n",
    "    return train_files, val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def pool_features(feature_mapping, selected_files):\n",
    "    \"\"\"\n",
    "    Pools all features and labels from the selected files into single arrays.\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_mapping (dict): Dictionary mapping file names to their features and labels.\n",
    "    - selected_files (list): List of file names to include.\n",
    "    \n",
    "    Returns:\n",
    "    - X (np.ndarray): Pooled feature arrays.\n",
    "    - y (np.ndarray): Pooled labels.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for file_name in selected_files:\n",
    "        data = feature_mapping.get(file_name)\n",
    "        if data is None:\n",
    "            print(f\"No data found for '{file_name}'. Skipping.\")\n",
    "            continue\n",
    "        X.append(data['features'])  # Each element has shape (num_segments, 40, PAD_LEN, 1)\n",
    "        y.extend([data['label']] * data['features'].shape[0])  # Repeat label for each segment\n",
    "    \n",
    "    if X:\n",
    "        X = np.concatenate(X, axis=0)  # Shape: (total_segments, 40, PAD_LEN, 1)\n",
    "    else:\n",
    "        X = np.array([])\n",
    "    \n",
    "    y = np.array(y)  # Shape: (total_segments,)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def verify_directory_structure(feature_dir, classes=[\"human\", \"ai\"], splits=[\"train\", \"validation\"]):\n",
    "    \"\"\"\n",
    "    Verifies the directory structure and file counts for training and validation datasets.\n",
    "    Prints details including the effective split percentage between train and validation for each class.\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_dir (str): Path to the base features directory (e.g., 'content/features').\n",
    "    - classes (list): List of class names (e.g., [\"human\", \"ai\"]).\n",
    "    - splits (list): List of data splits (e.g., [\"train\", \"validation\"]).\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print(f\"Verifying directory structure in '{feature_dir}'...\\n\")\n",
    "    \n",
    "    # Initialize dictionaries to hold counts\n",
    "    counts = {cls: {split: 0 for split in splits} for cls in classes}\n",
    "    total_counts = {cls: 0 for cls in classes}\n",
    "    \n",
    "    # Flag to track overall structure validity\n",
    "    structure_valid = True\n",
    "    \n",
    "    for split in splits:\n",
    "        for cls in classes:\n",
    "            split_cls_dir = os.path.join(feature_dir, split, cls)\n",
    "            \n",
    "            if not os.path.isdir(split_cls_dir):\n",
    "                print(f\"Missing directory: '{split_cls_dir}'\")\n",
    "                structure_valid = False\n",
    "                continue  # Skip counting if directory doesn't exist\n",
    "            \n",
    "            # List all .npz files in the directory\n",
    "            npz_files = [f for f in os.listdir(split_cls_dir) if f.lower().endswith('.npz')]\n",
    "            file_count = len(npz_files)\n",
    "            counts[cls][split] = file_count\n",
    "            total_counts[cls] += file_count\n",
    "            \n",
    "            print(f\"Directory: '{split_cls_dir}'\")\n",
    "            print(f\"  Number of .npz files: {file_count}\\n\")\n",
    "    \n",
    "    if not structure_valid:\n",
    "        print(\"Directory structure verification failed due to missing directories.\\n\")\n",
    "    else:\n",
    "        print(\"All required directories are present.\\n\")\n",
    "    \n",
    "    print(\"Effective Split Percentages:\\n\")\n",
    "    for cls in classes:\n",
    "        train_count = counts[cls][\"train\"]\n",
    "        val_count = counts[cls][\"validation\"]\n",
    "        total = total_counts[cls]\n",
    "        \n",
    "        if total == 0:\n",
    "            print(f\"Class '{cls}': No files found.\")\n",
    "            continue\n",
    "        \n",
    "        train_pct = (train_count / total) * 100\n",
    "        val_pct = (val_count / total) * 100\n",
    "        \n",
    "        print(f\"Class '{cls}':\")\n",
    "        print(f\"  Training: {train_count} files ({train_pct:.2f}%)\")\n",
    "        print(f\"  Validation: {val_count} files ({val_pct:.2f}%)\\n\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"Verification Summary:\")\n",
    "    for cls in classes:\n",
    "        total = total_counts[cls]\n",
    "        print(f\"Class '{cls}' - Total Files: {total}\")\n",
    "    print(\"\\nDirectory structure verification complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save precomputed features; skip over ones that already exist. Set force to True after changing segment length or other parameters.\n",
    "train_files, val_files = save_all_features(force=False)\n",
    "\n",
    "verify_directory_structure(\"content/features\", \"content/audio\")\n",
    "\n",
    "# === Load Precomputed Features ===\n",
    "features_mapping = load_features(FEATURES_CONTENT_DIR, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "OVP_Z-1w2xWV",
    "outputId": "42ee3457-d933-4497-d69b-489d204527da",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Features_mapping contains all files, so we must group them into validation and training\n",
    "\n",
    "X_train, y_train = pool_features(features_mapping, train_files)\n",
    "y_train_categorical = to_categorical(y_train, num_classes=2)\n",
    "print(f\"Pooled Training Features Shape: {X_train.shape}\")\n",
    "print(f\"Pooled Training Labels Shape: {y_train_categorical.shape}\")\n",
    "\n",
    "X_val, y_val = pool_features(features_mapping, val_files)\n",
    "y_val_categorical = to_categorical(y_val, num_classes=2)\n",
    "print(f\"Pooled Validation Features Shape: {X_val.shape}\")\n",
    "print(f\"Pooled Validation Labels Shape: {y_val_categorical.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if not LOAD_MODEL:\n",
    "    model = Sequential([\n",
    "        # First Convolutional Block\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(40, PAD_LEN, 1)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        Conv2D(filters=256, kernel_size=(3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=256, kernel_size=(3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Flatten and Dense Layers\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.7),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "else:\n",
    "    model = load_model(\"saved_models/21Jan2025.h5\")\n"
   ],
   "metadata": {
    "id": "dL9eDuUD2oKM",
    "tags": []
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not LOAD_MODEL: # Don't train the model if we are loading it\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        y_train_categorical,\n",
    "        epochs=25,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val_categorical),\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_file(file_name, model, features_mapping, pad_len=PAD_LEN, debug=False):\n",
    "    \"\"\"\n",
    "    Evaluates a single file and computes the AI certainty percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_name (str): Name of the file (without directory and extension).\n",
    "    - model (keras.Model): Trained Keras model for prediction.\n",
    "    - features_mapping (dict): Mapping of file names to their features and labels.\n",
    "    - pad_len (int): Fixed length for padding/truncating MFCCs.\n",
    "    - debug (bool): If True, prints debug information for each file processed.\n",
    "    \n",
    "    Returns:\n",
    "    - certainty (float): AI certainty percentage for the file.\n",
    "    \"\"\"\n",
    "    data = features_mapping.get(file_name)\n",
    "    if data is None:\n",
    "        found = False\n",
    "        for cls in classes:\n",
    "            audio_dir = os.path.join(AUDIO_CONTENT_DIR, cls)\n",
    "            audio_path_wav = os.path.join(audio_dir, f\"{file_name}.wav\")\n",
    "            audio_path_mp3 = os.path.join(audio_dir, f\"{file_name}.mp3\")\n",
    "            \n",
    "            if os.path.exists(audio_path_wav):\n",
    "                audio_path = audio_path_wav\n",
    "                label = classes.index(cls)\n",
    "                found = True\n",
    "                break\n",
    "            elif os.path.exists(audio_path_mp3):\n",
    "                audio_path = audio_path_mp3\n",
    "                label = classes.index(cls)\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"Audio file for '{file_name}' not found in any class directories.\")\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            audio, sample_rate = librosa.load(audio_path, res_type='kaiser_fast', mono=True)\n",
    "            segments = chop_audio(audio, sample_rate, SEGMENT_DURATION, HOP_LENGTH)\n",
    "            \n",
    "            features = []\n",
    "            for segment in segments:\n",
    "                # Do not apply loudness normalization for validation\n",
    "                mfcc = extract_features(segment, sample_rate, pad_len)\n",
    "                if mfcc is not None:\n",
    "                    features.append(mfcc.astype(np.float32)[..., np.newaxis])  # Shape: (40, PAD_LEN, 1)\n",
    "            \n",
    "            if not features:\n",
    "                print(f\"No features extracted for '{file_name}'.\")\n",
    "                return 0.0\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            features = np.array(features, dtype=np.float32)  # Shape: (num_segments, 40, PAD_LEN, 1)\n",
    "            \n",
    "            predictions = model.predict(features, batch_size=32, verbose=0)\n",
    "            predicted_classes = np.argmax(predictions, axis=1)  # Shape: (num_segments,)\n",
    "            \n",
    "            ai_segments = np.sum(predicted_classes == 1)\n",
    "            total_segments = len(predicted_classes)\n",
    "            certainty = (ai_segments / total_segments) * 100 if total_segments > 0 else 0\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"File: {file_name}, Class: {classes[label]}, AI Certainty: {certainty:.2f}%\")\n",
    "            \n",
    "            return certainty\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing '{file_name}': {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    else:\n",
    "        features = data['features']  # Shape: (num_segments, 40, PAD_LEN, 1)\n",
    "        label = data['label']        # 0 for Human, 1 for AI\n",
    "        \n",
    "        try:\n",
    "            # Predict classes for all segments in the file\n",
    "            predictions = model.predict(features, batch_size=32, verbose=0)\n",
    "            predicted_classes = np.argmax(predictions, axis=1)  # Shape: (num_segments,)\n",
    "            \n",
    "            # Calculate AI certainty\n",
    "            ai_segments = np.sum(predicted_classes == 1)\n",
    "            total_segments = len(predicted_classes)\n",
    "            certainty = (ai_segments / total_segments) * 100 if total_segments > 0 else 0\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"File: {file_name}, Class: {classes[label]}, AI Certainty: {certainty:.2f}%\")\n",
    "            \n",
    "            return certainty\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating file '{file_name}': {e}\")\n",
    "            return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not LOAD_MODEL:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_validation_files(model, val_files, features_mapping, pad_len=PAD_LEN, debug=False):\n",
    "    \"\"\"\n",
    "    Evaluates all files in the validation set and computes the distribution of AI certainty.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (keras.Model): Trained Keras model for prediction.\n",
    "    - val_files (list): List of validation file names (without directory and extension).\n",
    "    - features_mapping (dict): Mapping of file names to their features and labels.\n",
    "    - pad_len (int): Fixed length for padding/truncating MFCCs.\n",
    "    - debug (bool): If True, prints debug information for each file processed.\n",
    "    \n",
    "    Returns:\n",
    "    - human_file_results (list): AI certainty percentages for Human files.\n",
    "    - ai_file_results (list): AI certainty percentages for AI files.\n",
    "    \"\"\"\n",
    "    human_file_results = []\n",
    "    ai_file_results = []\n",
    "    \n",
    "    for file_name in val_files:\n",
    "        certainty = evaluate_file(file_name, model, features_mapping, pad_len=pad_len, debug=debug)\n",
    "        \n",
    "        label = features_mapping[file_name]['label'] if file_name in features_mapping else None\n",
    "        if label == 0:\n",
    "            human_file_results.append(certainty)\n",
    "        elif label == 1:\n",
    "            ai_file_results.append(certainty)\n",
    "        \n",
    "        file_type = 'Human' if label == 0 else 'AI' if label == 1 else 'Unknown'\n",
    "        print(f\"Processed File: {file_name}, Label: {file_type}, AI Certainty: {certainty:.2f}%\")\n",
    "    \n",
    "    # Compute Averages\n",
    "    average_human_certainty = np.mean(human_file_results) if human_file_results else 0\n",
    "    average_ai_certainty = np.mean(ai_file_results) if ai_file_results else 0\n",
    "    \n",
    "    # Print Results\n",
    "    print(\"\\n----- Evaluation Results -----\")\n",
    "    print(f\"Average AI Certainty for Human Files: {average_human_certainty:.2f}%\")\n",
    "    print(f\"Average AI Certainty for AI Files: {average_ai_certainty:.2f}%\")\n",
    "    \n",
    "    return human_file_results, ai_file_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_results, ai_results = evaluate_validation_files(\n",
    "    model, \n",
    "    val_files, \n",
    "    features_mapping, \n",
    "    pad_len=PAD_LEN, \n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Evaluate the model on validation data\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "loss, accuracy = model.evaluate(X_val, y_val_categorical, batch_size=32, verbose=1)\n",
    "print(f\"Validation Loss: {loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "labels = ['Human'] * len(human_results) + ['AI'] * len(ai_results)\n",
    "certainties = human_results + ai_results\n",
    "\n",
    "base_x = {'Human': 0.4, 'AI': 0.6}\n",
    "x_values = [base_x[label] for label in labels]\n",
    "\n",
    "jitter_strength = 0.01\n",
    "x_values_jittered = [x + np.random.uniform(-jitter_strength, jitter_strength) for x in x_values]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.scatter(\n",
    "    x_values_jittered, \n",
    "    certainties, \n",
    "    color='green', \n",
    "    alpha=0.4,         \n",
    "    edgecolors='none', \n",
    "    s=80               \n",
    ")\n",
    "\n",
    "plt.xticks([0.4, 0.6], ['Human', 'AI'], fontsize=18)\n",
    "\n",
    "plt.xlabel('Label', fontsize=16)\n",
    "plt.ylabel('AI Certainty (%)', fontsize=16)\n",
    "plt.title('Distribution of AI Certainty for Human and AI Music Files (Validation)', fontsize=18)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xlim(0.2, 0.8)\n",
    "\n",
    "# Move the annotation to the right side, just outside the plot\n",
    "plt.text(0.82, 50, '1 dot = 1 song', rotation=90, fontsize=14, va='center', ha='left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "human_df = pd.DataFrame({\n",
    "    'file_type': ['human'] * len(human_results),\n",
    "    'ai_certainty_percentage': human_results\n",
    "})\n",
    "\n",
    "ai_df = pd.DataFrame({\n",
    "    'file_type': ['ai'] * len(ai_results),\n",
    "    'ai_certainty_percentage': ai_results\n",
    "})\n",
    "\n",
    "combined_df = pd.concat([human_df, ai_df], ignore_index=True)\n",
    "\n",
    "from datetime import date\n",
    "combined_df.to_csv(str(date.today()) + \"_\" + date.today().strftime(\"%b-%d-%Y\") + '_data.csv', index=False)\n",
    "\n",
    "print(\"Results have been saved\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_segment_bars(segments, predicted_classes, file_name, confidence, gap=0.01):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    n = len(segments)\n",
    "    if n == 0:\n",
    "        print(\"No segments to plot.\")\n",
    "        return\n",
    "\n",
    "    random_heights = np.random.uniform(20, 50, n)\n",
    "    total_width = 1.0\n",
    "    bar_width = (total_width - (n + 1) * gap) / n\n",
    "    x_positions = [gap + i*(bar_width + gap) + bar_width/2 for i in range(n)]\n",
    "\n",
    "    colors = ['red' if p == 1 else 'gray' for p in predicted_classes]\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for x, height, color in zip(x_positions, random_heights, colors):\n",
    "        plt.bar(x, height, width=bar_width/(n/10), color=color, bottom=-height/2, align='center')\n",
    "\n",
    "    xticks = x_positions\n",
    "    times = [i * HOP_LENGTH for i in range(n)]\n",
    "    \n",
    "    time_labels = [f\"{int(t//60):02}:{int(t%60):02}\" for t in times]\n",
    "    filtered_xticks = [xticks[i] for i in range(n) if times[i] % 20 == 0]\n",
    "    filtered_time_labels = [time_labels[i] for i in range(n) if times[i] % 20 == 0]\n",
    "    \n",
    "    plt.xticks(filtered_xticks, filtered_time_labels, rotation=45)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"Time (mm:ss)\")\n",
    "    plt.title(file_name + \" | AI Certainty: \" + f\"{confidence:.2f}%\")\n",
    "    \n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='gray', lw=5, label='Human'),\n",
    "        Line2D([0], [0], color='red', lw=5, label='AI')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements)\n",
    "    \n",
    "    plt.xlim(0, total_width)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_independent_file(file_path, model, classes=classes, \n",
    "                              SEGMENT_DURATION=SEGMENT_DURATION, HOP_LENGTH=HOP_LENGTH, \n",
    "                              PAD_LEN=PAD_LEN, debug=False, image_gen=True):\n",
    "    \"\"\"\n",
    "    Evaluates an independent audio file by converting it to mono 64kbps mp3, extracting MFCC features \n",
    "    (without normalization), computing the AI certainty percentage using the provided model, and (if image_gen=True) \n",
    "    calling plot_segment_bars to visualize the segments.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the input audio file.\n",
    "    - model (keras.Model): Trained Keras model for prediction.\n",
    "    - classes (list): List of class names (e.g., [\"human\", \"ai\"]).\n",
    "    - SEGMENT_DURATION (float): Duration of each audio segment in seconds.\n",
    "    - HOP_LENGTH (float): Hop length in seconds between segments.\n",
    "    - PAD_LEN (int): Fixed length for MFCC padding/truncating.\n",
    "    - debug (bool): If True, prints debug information.\n",
    "    - image_gen (bool): If True, calls plot_segment_bars to display the visualization.\n",
    "    \n",
    "    Returns:\n",
    "    - certainty (float): AI certainty percentage for the file.\n",
    "    \"\"\"\n",
    "    import tempfile, subprocess, os, numpy as np, librosa\n",
    "    \n",
    "    try:\n",
    "        # Create a temporary file for the converted mp3\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False) as tmp:\n",
    "            temp_mp3_path = tmp.name\n",
    "        \n",
    "        # Convert to mono 64kbps mp3 using ffmpeg\n",
    "        ffmpeg_command = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", file_path,\n",
    "            \"-ar\", \"22050\",      # Sample rate\n",
    "            \"-ac\", \"1\",          # Mono\n",
    "            \"-b:a\", \"64k\",       # Bitrate\n",
    "            \"-y\",                # Overwrite if exists\n",
    "            temp_mp3_path\n",
    "        ]\n",
    "        subprocess.run(ffmpeg_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
    "        \n",
    "        audio, sample_rate = librosa.load(temp_mp3_path, sr=22050, mono=True)\n",
    "        \n",
    "        num_samples_per_segment = int(SEGMENT_DURATION * sample_rate)\n",
    "        hop_length_samples = int(HOP_LENGTH * sample_rate)\n",
    "        segments = []\n",
    "        for start in range(0, len(audio) - num_samples_per_segment + 1, hop_length_samples):\n",
    "            end = start + num_samples_per_segment\n",
    "            segments.append(audio[start:end])\n",
    "        \n",
    "        # Extract features for each segment without normalization\n",
    "        features = []\n",
    "        for segment in segments:\n",
    "            mfcc = extract_features(segment, sample_rate, PAD_LEN)\n",
    "            if mfcc is not None:\n",
    "                features.append(mfcc.astype(np.float32)[..., np.newaxis])\n",
    "        \n",
    "        if not features:\n",
    "            print(f\"No features extracted for '{file_path}'.\")\n",
    "            return 0.0\n",
    "        \n",
    "        features = np.array(features, dtype=np.float32)\n",
    "        \n",
    "        predictions = model.predict(features, batch_size=32, verbose=0)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        # Calculate AI certainty\n",
    "        ai_segments = np.sum(predicted_classes == 1)\n",
    "        total_segments = len(predicted_classes)\n",
    "        certainty = (ai_segments / total_segments) * 100 if total_segments > 0 else 0\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"File: {file_path}, AI Certainty: {certainty:.2f}%\")\n",
    "        \n",
    "        # Generate the visualization using plot_segment_bars if image_gen is True\n",
    "        if image_gen:\n",
    "            plot_segment_bars(segments, predicted_classes, os.path.splitext(os.path.basename(file_path))[0], certainty, gap=0.05)\n",
    "        \n",
    "        return certainty\n",
    "    \n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Error: ffmpeg failed to convert '{file_path}'.\")\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing '{file_path}': {e}\")\n",
    "        return 0.0\n",
    "    finally:\n",
    "        if os.path.exists(temp_mp3_path):\n",
    "            os.remove(temp_mp3_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "evaluate_independent_file(\"content/misc/youtube_sample.mp3\", model, debug=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "evaluate_independent_file(\"content/misc/youtube_sample.mp3\", model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
